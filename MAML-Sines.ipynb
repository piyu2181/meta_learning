{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from src.sine_tasks import Sine_Task, Sine_Task_Distribution\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "To implement MAML in Pytorch, we will need a model that can be easily parametrised with a set of weights that's distinct from the model's own parameters. Having these weights distinct from the model.parameters() allows us to easily make differentiable gradient updates within the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "        self.modela = nn.Sequential(OrderedDict([\n",
    "            ('l2a', nn.Linear(40,40)),\n",
    "            ('relu2a', nn.ReLU()),\n",
    "            ('l3a', nn.Linear(40,1))\n",
    "        ]))\n",
    "        self.modelb = nn.Sequential(OrderedDict([\n",
    "            ('l2b', nn.Linear(40,40)),\n",
    "            ('relu2b', nn.ReLU()),\n",
    "            ('l3b', nn.Linear(40,1))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.modela(self.model(x)), self.modelb(self.model(x))\n",
    "    \n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        x = nn.functional.linear(x, weights[0], weights[1])\n",
    "        x = nn.functional.relu(x)\n",
    "        xa = nn.functional.linear(x, weights[2], weights[3])\n",
    "        xa = nn.functional.relu(xa)\n",
    "        xa = nn.functional.linear(xa, weights[4], weights[5])\n",
    "        xb = nn.functional.linear(x, weights[6], weights[7])\n",
    "        xb = nn.functional.relu(xb)\n",
    "        xb = nn.functional.linear(xb, weights[8], weights[9])\n",
    "        return xa, xb                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj = MAMLModel()\n",
    "#print(obj.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML\n",
    "\n",
    "This code implements MAML for supervised few-shot learning.\n",
    "\n",
    "It is probably not performance-optimal, but was designed to reproduce the behaviour in the paper while being maximally simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML():\n",
    "    def __init__(self, model, tasks, inner_lr, meta_lr, K=10, inner_steps=1, tasks_per_meta_batch=1000):\n",
    "        \n",
    "        # important objects\n",
    "        self.tasks = tasks\n",
    "        self.model = model\n",
    "        self.weights = list(model.parameters())\n",
    "        #self.weightsa = list(modela.parameters())\n",
    "        #self.weightsb = list(modelb.parameters())\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = torch.optim.Adam(self.weights, meta_lr)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps \n",
    "        self.tasks_per_meta_batch = tasks_per_meta_batch \n",
    "        \n",
    "        # metrics\n",
    "        self.plot_every = 100\n",
    "        self.print_every = 100\n",
    "        self.meta_losses = []\n",
    "        self.meta_y_losses = []\n",
    "        self.meta_noise_losses = []\n",
    "    \n",
    "    def inner_loop(self, task):\n",
    "        # reset inner model to current maml weights\n",
    "        temp_weights = [w.clone() for w in self.weights]\n",
    "        \n",
    "        # perform training on data sampled from task\n",
    "        X, y, y2 = task.sample_data(True,self.K)\n",
    "        y_noise = (y2 - y)\n",
    "        for step in range(self.inner_steps):\n",
    "            #print(self.model.parameterised(X, temp_weights)[0])\n",
    "            y_loss = self.criterion(self.model.parameterised(X, temp_weights)[0], y) / self.K # kind of training loss\n",
    "            noise_loss = self.criterion(self.model.parameterised(X, temp_weights)[1], y_noise) / self.K # kind of training loss\n",
    "            final_loss = (y_loss + noise_loss)\n",
    "            \n",
    "            # compute grad and update inner loop weights\n",
    "            grad =torch.autograd.grad(final_loss, temp_weights)\n",
    "            #grad_noise =torch.autograd.grad(noise_loss, temp_noise_weights)\n",
    "            temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "            \n",
    "        \n",
    "        #sample new data for meta-update and compute loss\n",
    "        X, y, y2 =  task.sample_data(True, self.K)\n",
    "        y_noise = (y2 - y)\n",
    "       \n",
    "        y_loss = self.criterion(self.model.parameterised(X, temp_weights)[0], y) / self.K #(kind of validation loss)\n",
    "        noise_loss = self.criterion(self.model.parameterised(X, temp_weights)[1], y_noise) / self.K\n",
    "        final_loss = y_loss + noise_loss\n",
    "        return (final_loss, y_loss.item(), noise_loss.item())\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        epoch_loss = 0\n",
    "        epoch_y_loss = 0\n",
    "        epoch_noise_loss = 0\n",
    "        \n",
    "        for iteration in range(1, num_iterations+1):\n",
    "            \n",
    "            # compute meta loss\n",
    "            meta_loss = 0\n",
    "            meta_y_loss = 0\n",
    "            meta_noise_loss = 0\n",
    "            \n",
    "            for i in range(self.tasks_per_meta_batch):\n",
    "                task = self.tasks.sample_task()\n",
    "                a,b,c = self.inner_loop(task)\n",
    "                meta_loss += a\n",
    "                meta_y_loss += b\n",
    "                meta_noise_loss += c\n",
    "            \n",
    "            # compute meta gradient of loss with respect to maml weights\n",
    "            meta_grads = torch.autograd.grad(meta_loss, self.weights)\n",
    "            \n",
    "            # assign meta gradient to weights and take optimisation step\n",
    "            for w, g in zip(self.weights, meta_grads):\n",
    "                w.grad = g\n",
    "            self.meta_optimiser.step()\n",
    "            \n",
    "            \n",
    "            # log metrics\n",
    "            epoch_loss += meta_loss.item() / self.tasks_per_meta_batch\n",
    "            epoch_y_loss += meta_y_loss / self.tasks_per_meta_batch\n",
    "            epoch_noise_loss += meta_noise_loss / self.tasks_per_meta_batch\n",
    "            \n",
    "            if iteration % self.print_every == 0:\n",
    "                print(\"{}/{}. loss: {}\".format(iteration, num_iterations, epoch_loss / self.plot_every))\n",
    "                print(\"{}/{}. y_loss: {}\".format(iteration, num_iterations, epoch_y_loss / self.plot_every))\n",
    "                print(\"{}/{}. noise_loss: {}\".format(iteration, num_iterations, epoch_noise_loss / self.plot_every))\n",
    "                \n",
    "            \n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(epoch_loss / self.plot_every)\n",
    "                self.meta_y_losses.append(epoch_y_loss / self.plot_every)\n",
    "                self.meta_noise_losses.append(epoch_noise_loss / self.plot_every)\n",
    "                epoch_loss = 0\n",
    "                epoch_y_loss = 0\n",
    "                epoch_noise_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4bd616da35c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSine_Task_Distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAMLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MAML' is not defined"
     ]
    }
   ],
   "source": [
    "tasks = Sine_Task_Distribution(0.1, 5, 0, np.pi, 0.1, 2, 0, (np.pi), -5, 5)\n",
    "maml = MAML(MAMLModel(), tasks, inner_lr=0.001, meta_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3f731f9aadb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-febe6eebff9c>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(self, num_iterations)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# compute meta gradient of loss with respect to maml weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mmeta_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# assign meta gradient to weights and take optimisation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maml.main_loop(num_iterations=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6c9976610>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maml.meta_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAML has a peculiar learning curve. At least on this task distirbution, the brief point of inflection (at 200 on the plot) seems to persist across different hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Plotting\n",
    "\n",
    "Now that the model is trained, let's look at how it performs, and compare it to some naive benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(maml.model.state_dict(), 'model.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_on_random_task(initial_model, K, num_steps, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    trains the model on a random sine task and measures the loss curve.\n",
    "    \n",
    "    for each n in num_steps_measured, records the model function after n gradient updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy MAML model into a new object to preserve MAML weights during training\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2a', nn.Linear(40,40)),\n",
    "            ('relu2a', nn.ReLU()),\n",
    "            ('l3a', nn.Linear(40,1)),\n",
    "            ('l2b', nn.Linear(40,40)),\n",
    "            ('relu2b', nn.ReLU()),\n",
    "            ('l3b', nn.Linear(40,1))\n",
    "        ]))\n",
    "    model.load_state_dict(initial_model.state_dict())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), 0.01)\n",
    "\n",
    "    # train model on a random task\n",
    "    task = tasks.sample_task()\n",
    "    X, y, y2 = task.sample_data(K)\n",
    "    y_noise = (y2 - y)\n",
    "    losses = []\n",
    "    noise_losses = []\n",
    "    total_losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X)[0], y) / K\n",
    "        noise_loss = criterion(model(X)[1], y_noise) / K\n",
    "        total_loss = (loss + noise_loss)\n",
    "        total_losses.append(total_loss.item())\n",
    "        losses.append(loss.item())\n",
    "        noise_losses.append(noise_loss.item())\n",
    "\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    return total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_losses(initial_model, n_samples, K=10, n_steps=10, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    returns the average learning trajectory of the model trained for ``n_iterations`` over ``n_samples`` tasks\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.linspace(-5, 5, 2) # dummy input for test_on_new_task\n",
    "    avg_losses = [0] * K\n",
    "    for i in range(n_samples):\n",
    "        losses = loss_on_random_task(initial_model, K, n_steps, optim)\n",
    "        avg_losses = [l + l_new for l, l_new in zip(avg_losses, losses)]\n",
    "    avg_losses = [l / n_samples for l in avg_losses]\n",
    "    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_pretrained(iterations=500):\n",
    "    \"\"\"\n",
    "    returns a model pretrained on a selection of ``iterations`` random tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up model\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "    modela = nn.Sequential(OrderedDict([\n",
    "        ('l2a', nn.Linear(40,40)),\n",
    "        ('relu2a', nn.ReLU()),\n",
    "        ('l3a', nn.Linear(40,1))\n",
    "    ]))\n",
    "    modelb = nn.Sequential(OrderedDict([\n",
    "        ('l2b', nn.Linear(40,40)),\n",
    "        ('relu2b', nn.ReLU()),\n",
    "        ('l3b', nn.Linear(40,1))\n",
    "    ]))\n",
    "    params = list(model.parameters())+ list(modela.parameters())+ list(modelb.parameters())\n",
    "    optimiser = torch.optim.Adam(params, lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # fit the model\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        modela.zero_grad()\n",
    "        modelb.zero_grad()\n",
    "        x, y, y2 = tasks.sample_task().sample_data(True, 10)\n",
    "        y_noise = (y2-y)\n",
    "        loss = criterion(modela(model(x)), y)\n",
    "        noise_loss = criterion(modelb(model(x)), y_noise)\n",
    "        total_loss = loss + noise_loss\n",
    "        total_loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    return model, modela, modelb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7bdb2a84a0ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixed_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-cdd11fa2f2d6>\u001b[0m in \u001b[0;36mmixed_pretrained\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodela\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mnoise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrained, pretraineda, pretrainedb = mixed_pretrained(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 1], m2: [40 x 40] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/TH/generic/THTensorMath.cpp:752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d05d9742f9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average learning trajectory for K=10, starting from initial weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient steps taken with SGD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-756c8d5dc45f>\u001b[0m in \u001b[0;36maverage_losses\u001b[0;34m(initial_model, n_samples, K, n_steps, optim)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mavg_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_on_random_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mavg_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml_new\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_new\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mavg_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavg_losses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a877b9d4b338>\u001b[0m in \u001b[0;36mloss_on_random_task\u001b[0;34m(initial_model, K, num_steps, optim)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtotal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mnoise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_noise\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 1], m2: [40 x 40] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/TH/generic/THTensorMath.cpp:752"
     ]
    }
   ],
   "source": [
    "plt.plot(average_losses(maml.model.model, n_samples=5000, K=10), label='maml')\n",
    "plt.plot(average_losses(pretrained,       n_samples=5000, K=10), label='pretrained')\n",
    "plt.legend()\n",
    "plt.title(\"Average learning trajectory for K=10, starting from initial weights\")\n",
    "plt.xlabel(\"gradient steps taken with SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which looks, modulo scale (probably because I divide loss by K) and the learning rate for pretrained, exactly like the plot from the paper.\n",
    "\n",
    "It is interesting to note that while the MAML weights do not overfit to only work after 1 step, they <i>do</i> fit, if not overfit, to the particular optimiser used in the inner step. Using Adam instead of SGD for the test-time optimiser results in much worse performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_losses(maml.model.model, n_samples=5000, K=10, optim=torch.optim.Adam), label='maml')\n",
    "plt.plot(average_losses(pretrained,       n_samples=5000, K=10, optim=torch.optim.Adam), label='pretrained')\n",
    "plt.legend()\n",
    "plt.title(\"Average learning trajectory for K=10, starting from initial weights\")\n",
    "plt.xlabel(\"gradient steps taken with Adam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_functions_at_training(initial_model, X, y, y2, sampled_steps, x_axis, optim=torch.optim.SGD, lr=0.01):\n",
    "    \"\"\"\n",
    "    trains the model on X, y and measures the loss curve.\n",
    "    \n",
    "    for each n in sampled_steps, records model(x_axis) after n gradient updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy MAML model into a new object to preserve MAML weights during training\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2a', nn.Linear(40,40)),\n",
    "            ('relu2a', nn.ReLU()),\n",
    "            ('l3a', nn.Linear(40,1)),\n",
    "            ('l2b', nn.Linear(40,40)),\n",
    "            ('relu2b', nn.ReLU()),\n",
    "            ('l3b', nn.Linear(40,1))\n",
    "        ]))\n",
    "    model.load_state_dict(initial_model.state_dict())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), lr)\n",
    "\n",
    "    # train model on a random task\n",
    "    num_steps = max(sampled_steps)\n",
    "    K = X.shape[0]\n",
    "    y_noise = (y2-y)\n",
    "    losses = []\n",
    "    total_losses = []\n",
    "    outputs = {}\n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X)[0], y) / K\n",
    "        noise_loss = criterion(model(X)[1], y_noise) / K\n",
    "        total_loss = (loss + noise_loss)\n",
    "        total_losses.append(total_loss.item())\n",
    "        losses.append(loss)\n",
    "        noise_losses.append(noise_loss)\n",
    "\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # plot the model function\n",
    "        if step in sampled_steps:\n",
    "            outputs[step] = model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "            \n",
    "    outputs['initial'] = initial_model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "    \n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sampled_performance(initial_model, model_name, task, X, y, optim=torch.optim.SGD, lr=0.01):\n",
    "    \n",
    "    x_axis = np.linspace(-5, 5, 1000)\n",
    "    sampled_steps=[1,10]\n",
    "    outputs, losses = model_functions_at_training(initial_model, \n",
    "                                                  X, y, \n",
    "                                                  sampled_steps=sampled_steps, \n",
    "                                                  x_axis = x_axis, \n",
    "                                                  optim=optim, lr=lr)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    # plot the model functions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    plt.plot(x_axis, task.true_function(x_axis), '-', color=(0, 0, 1, 0.5), label='true function')\n",
    "    plt.scatter(X, y, label='data')\n",
    "    plt.plot(x_axis, outputs['initial'], ':', color=(0.7, 0, 0, 1), label='initial weights')\n",
    "    \n",
    "    for step in sampled_steps:\n",
    "        plt.plot(x_axis, outputs[step], \n",
    "                 '-.' if step == 1 else '-', color=(0.5, 0, 0, 1),\n",
    "                 label='model after {} steps'.format(step))\n",
    "        \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(\"Model fit: {}\".format(model_name))\n",
    "\n",
    "    # plot losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss over time\")\n",
    "    plt.xlabel(\"gradient steps taken\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "task = tasks.sample_task()\n",
    "X, y = task.sample_data(K)\n",
    "\n",
    "plot_sampled_performance(maml.model.model, 'MAML', task, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampled_performance(pretrained, 'pretrained at lr=0.02', task, X, y, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on K=5 is also good, although less reliable than K=10. This is unsurprising, as we train MAML with K=10. \n",
    "\n",
    "It <i>is</i> surprising that despite the uneven loss curves, the model at 10 steps is often well-fit, despite very sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "task = tasks.sample_task()\n",
    "X, y = task.sample_data(K)\n",
    "\n",
    "plot_sampled_performance(maml.model.model, 'MAML', task, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
